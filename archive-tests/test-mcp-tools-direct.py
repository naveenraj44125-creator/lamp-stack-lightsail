#!/usr/bin/env python3
"""
Direct MCP Tools Test - Test the enhanced MCP server tools
Tests the server's ability to handle blueprint_id and bundle_id parameters
"""

import requests
import json
import subprocess
import tempfile
import os
import sys
from datetime import datetime

MCP_SERVER_URL = "http://18.215.231.164:3000"

def test_server_health():
    """Test server health and version"""
    print("ğŸ” Testing MCP server health and version...")
    
    try:
        response = requests.get(f"{MCP_SERVER_URL}/health", timeout=10)
        if response.status_code == 200:
            health_data = response.json()
            version = health_data.get('version', 'unknown')
            print(f"âœ… MCP server is healthy - Version: {version}")
            return version == '1.1.0'  # Check for latest version
        else:
            print(f"âŒ Health check failed: HTTP {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Health check failed: {e}")
        return False

def test_enhanced_landing_page_content():
    """Test the enhanced landing page content in detail"""
    print("\nğŸŒ Testing enhanced landing page content...")
    
    try:
        response = requests.get(f"{MCP_SERVER_URL}/", timeout=10)
        if response.status_code == 200:
            content = response.text.lower()
            
            # More comprehensive checks
            checks = [
                ("Multi-OS Support", any(term in content for term in ["ubuntu", "amazon linux", "centos"])),
                ("Instance Sizes", any(term in content for term in ["nano", "micro", "small", "medium", "large"])),
                ("OS Selection", "multiple os" in content or "operating system" in content),
                ("Size Selection", "instance size" in content or "2xlarge" in content),
                ("Enhanced Tools", "setup_new_repository" in content),
                ("Integration Tool", "integrate_lightsail_actions" in content or "existing repositories" in content),
                ("Pricing Info", any(term in content for term in ["$", "pricing", "cost"])),
                ("Interactive Config", "interactive" in content)
            ]
            
            passed = 0
            for check_name, check_result in checks:
                status = "âœ…" if check_result else "âŒ"
                print(f"   {status} {check_name}")
                if check_result:
                    passed += 1
            
            success_rate = passed / len(checks)
            print(f"âœ… Landing page features: {passed}/{len(checks)} ({success_rate*100:.1f}%)")
            return success_rate >= 0.75  # 75% pass rate
        else:
            print(f"âŒ Landing page failed: HTTP {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Landing page test failed: {e}")
        return False

def test_script_parameter_support():
    """Test if the setup scripts support the new parameters"""
    print("\nğŸ”§ Testing script parameter support...")
    
    # Test setup-new-repo.sh
    try:
        with open('setup-new-repo.sh', 'r') as f:
            setup_content = f.read()
        
        # Check for environment variable support
        env_vars = ['BLUEPRINT_ID', 'BUNDLE_ID', 'APP_TYPE', 'INSTANCE_NAME']
        supported_vars = sum(1 for var in env_vars if var in setup_content)
        
        print(f"   âœ… setup-new-repo.sh: {supported_vars}/{len(env_vars)} environment variables supported")
        
        # Check for OS options
        os_options = ['ubuntu_22_04', 'ubuntu_20_04', 'amazon_linux_2023', 'centos_7']
        supported_os = sum(1 for os_opt in os_options if os_opt in setup_content)
        
        print(f"   âœ… OS options: {supported_os}/{len(os_options)} operating systems supported")
        
        # Check for bundle options
        bundle_options = ['nano_3_0', 'micro_3_0', 'small_3_0', 'medium_3_0', 'large_3_0']
        supported_bundles = sum(1 for bundle in bundle_options if bundle in setup_content)
        
        print(f"   âœ… Bundle options: {supported_bundles}/{len(bundle_options)} instance sizes supported")
        
        setup_score = (supported_vars + supported_os + supported_bundles) / (len(env_vars) + len(os_options) + len(bundle_options))
        
    except Exception as e:
        print(f"   âŒ setup-new-repo.sh test failed: {e}")
        setup_score = 0
    
    # Test integrate-lightsail-actions.sh
    try:
        with open('integrate-lightsail-actions.sh', 'r') as f:
            integrate_content = f.read()
        
        # Similar checks for integration script
        supported_vars_int = sum(1 for var in env_vars if var in integrate_content)
        supported_os_int = sum(1 for os_opt in os_options if os_opt in integrate_content)
        supported_bundles_int = sum(1 for bundle in bundle_options if bundle in integrate_content)
        
        print(f"   âœ… integrate-lightsail-actions.sh: {supported_vars_int}/{len(env_vars)} environment variables supported")
        print(f"   âœ… OS options: {supported_os_int}/{len(os_options)} operating systems supported")
        print(f"   âœ… Bundle options: {supported_bundles_int}/{len(bundle_options)} instance sizes supported")
        
        integrate_score = (supported_vars_int + supported_os_int + supported_bundles_int) / (len(env_vars) + len(os_options) + len(bundle_options))
        
    except Exception as e:
        print(f"   âŒ integrate-lightsail-actions.sh test failed: {e}")
        integrate_score = 0
    
    overall_score = (setup_score + integrate_score) / 2
    print(f"   ğŸ¯ Overall script support: {overall_score*100:.1f}%")
    
    return overall_score >= 0.8  # 80% pass rate

def test_mcp_server_tool_simulation():
    """Simulate MCP server tool execution"""
    print("\nğŸ› ï¸ Testing MCP server tool simulation...")
    
    # Since we can't easily test MCP protocol directly, we'll test the underlying functionality
    # by simulating what the MCP server would do internally
    
    test_configs = [
        {
            "name": "Ubuntu 22.04 + Small",
            "blueprint_id": "ubuntu_22_04",
            "bundle_id": "small_3_0",
            "app_type": "nodejs"
        },
        {
            "name": "Amazon Linux 2023 + Medium",
            "blueprint_id": "amazon_linux_2023", 
            "bundle_id": "medium_3_0",
            "app_type": "python"
        }
    ]
    
    results = []
    for config in test_configs:
        print(f"   ğŸ§ª Testing configuration: {config['name']}")
        
        # Create a temporary script to test parameter passing
        script_content = f"""#!/bin/bash
export BLUEPRINT_ID="{config['blueprint_id']}"
export BUNDLE_ID="{config['bundle_id']}"
export APP_TYPE="{config['app_type']}"
export INSTANCE_NAME="test-instance"
export AWS_REGION="us-east-1"

echo "Configuration test:"
echo "  Blueprint: $BLUEPRINT_ID"
echo "  Bundle: $BUNDLE_ID"
echo "  App Type: $APP_TYPE"
echo "  Instance: $INSTANCE_NAME"
echo "  Region: $AWS_REGION"

# Test if the setup script would accept these parameters
if [ -f "setup-new-repo.sh" ]; then
    echo "âœ… setup-new-repo.sh found"
else
    echo "âŒ setup-new-repo.sh not found"
    exit 1
fi

echo "âœ… Parameter simulation successful"
"""
        
        try:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.sh', delete=False) as f:
                f.write(script_content)
                temp_script = f.name
            
            os.chmod(temp_script, 0o755)
            result = subprocess.run([temp_script], capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0:
                print(f"      âœ… Configuration test passed")
                results.append(True)
            else:
                print(f"      âŒ Configuration test failed: {result.stderr}")
                results.append(False)
            
            os.unlink(temp_script)
            
        except Exception as e:
            print(f"      âŒ Configuration test error: {e}")
            results.append(False)
    
    success_rate = sum(results) / len(results) if results else 0
    print(f"   ğŸ¯ Configuration tests: {sum(results)}/{len(results)} passed ({success_rate*100:.1f}%)")
    
    return success_rate >= 0.8

def test_deployment_config_generation():
    """Test deployment configuration generation with new parameters"""
    print("\nğŸ“ Testing deployment configuration generation...")
    
    # Test different configuration combinations
    test_cases = [
        {
            "app_type": "nodejs",
            "blueprint_id": "ubuntu_22_04",
            "bundle_id": "small_3_0",
            "expected_os": "Ubuntu 22.04 LTS",
            "expected_size": "Small (2GB)"
        },
        {
            "app_type": "python",
            "blueprint_id": "amazon_linux_2023",
            "bundle_id": "medium_3_0", 
            "expected_os": "Amazon Linux 2023",
            "expected_size": "Medium (4GB)"
        }
    ]
    
    results = []
    for case in test_cases:
        print(f"   ğŸ§ª Testing {case['app_type']} on {case['blueprint_id']} ({case['bundle_id']})")
        
        # Check if we have a deployment config template
        config_file = f"deployment-{case['app_type']}.config.yml"
        try:
            with open(config_file, 'r') as f:
                config_content = f.read()
            
            # Check if config supports the new parameters
            has_blueprint = 'blueprint_id' in config_content
            has_bundle = 'bundle_id' in config_content
            has_auto_create = 'auto_create' in config_content
            
            score = sum([has_blueprint, has_bundle, has_auto_create])
            results.append(score >= 2)  # At least 2/3 features
            
            print(f"      âœ… Config features: {score}/3 (blueprint: {has_blueprint}, bundle: {has_bundle}, auto_create: {has_auto_create})")
            
        except FileNotFoundError:
            print(f"      âš ï¸  Config file {config_file} not found")
            results.append(False)
        except Exception as e:
            print(f"      âŒ Config test failed: {e}")
            results.append(False)
    
    success_rate = sum(results) / len(results) if results else 0
    print(f"   ğŸ¯ Config generation tests: {sum(results)}/{len(results)} passed ({success_rate*100:.1f}%)")
    
    return success_rate >= 0.8

def run_direct_mcp_test():
    """Run direct MCP server functionality test"""
    print("ğŸ§ª Direct MCP Server Tools Test - Enhanced Functionality")
    print("=" * 55)
    print(f"ğŸ•’ Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸŒ Testing server: {MCP_SERVER_URL}")
    print()
    
    test_results = {}
    
    # Test 1: Server Health & Version
    test_results['health'] = test_server_health()
    
    # Test 2: Enhanced Landing Page
    test_results['landing_page'] = test_enhanced_landing_page_content()
    
    # Test 3: Script Parameter Support
    test_results['script_support'] = test_script_parameter_support()
    
    # Test 4: MCP Tool Simulation
    test_results['tool_simulation'] = test_mcp_server_tool_simulation()
    
    # Test 5: Config Generation
    test_results['config_generation'] = test_deployment_config_generation()
    
    # Summary
    print("\nğŸ“Š Direct MCP Test Results")
    print("=" * 30)
    
    passed_tests = sum(1 for result in test_results.values() if result)
    total_tests = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status} {test_name.replace('_', ' ').title()}")
    
    success_rate = passed_tests/total_tests*100
    print(f"\nğŸ¯ Overall Result: {passed_tests}/{total_tests} tests passed ({success_rate:.1f}%)")
    
    if passed_tests == total_tests:
        print("ğŸ‰ All direct tests passed! MCP server enhanced functionality is working perfectly.")
        print("âœ¨ Confirmed capabilities:")
        print("   â€¢ Server running latest version (1.1.0)")
        print("   â€¢ Enhanced landing page with multi-OS and sizing info")
        print("   â€¢ Scripts support blueprint_id and bundle_id parameters")
        print("   â€¢ Configuration generation with new parameters")
        print("   â€¢ Tool simulation successful")
    elif passed_tests >= total_tests * 0.8:
        print("âš ï¸  Most direct tests passed. Enhanced functionality is mostly working.")
        print("ğŸ”§ Minor issues detected - check failed tests above.")
    else:
        print("âŒ Multiple direct test failures. Enhanced functionality needs attention.")
    
    print(f"\nğŸ•’ Test completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    return test_results

if __name__ == "__main__":
    results = run_direct_mcp_test()
    
    # Exit with appropriate code
    if all(results.values()):
        sys.exit(0)  # Success
    else:
        sys.exit(1)  # Some tests failed